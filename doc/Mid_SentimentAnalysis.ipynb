{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중간과제: 텍스트마이닝을 통한 정서분류\n",
    "\n",
    "## 1. 과제개요\n",
    "* 정서: 사람의 마음에 일어나는 여러 가지 감정. 또는 감정을 불러일으키는 기분이나 분위기.\n",
    "* 특정 주제에 대해 웹 상에 있는 의견을 모아 긍정(Positive) 혹은 부정(Negative)로 분류한다.\n",
    "* 제출데이터는 데이터, 보고서(가설, 연구방법, 결론), 실행프로그램으로 구성한다.\n",
    "* 데이터는 웹에서 300개의 문장을 수집하며, Training, Test로 나누어 진행한다.\n",
    "\n",
    "## 2. 텍스트마이닝을 통한 정서분류 개요\n",
    "* 주제: 서울 지하철에 대한 외국인의 정서\n",
    "* 가설: 서울 지하철에 대한 외국인의 정서를 긍정 or 부정으로 분류할 수 있다.\n",
    "\n",
    "## 3. 연구방법\n",
    "\n",
    "### 3-1. 데이터 마이닝\n",
    "* Training Set을 확보하기 위해 트립어드바이저 리뷰에서 데이터 마이닝을 수동으로 하였다.\n",
    "* 트립어드바이저( https://www.tripadvisor.co.kr/Attraction_Review-g294197-d2194168-Reviews-Seoul_Metro-Seoul.html )\n",
    "* '아주좋음', '좋음' 평점을 준 리뷰에서 Positive 문장 150개를 긁어 MetroPositiveTraining 텍스트 파일로 저장하였다.\n",
    "* '보통', '별로', '최악' 평점을 준 리뷰에서 Negative 문장 150개를 긁어 MetroNegativeTraining 텍스트 파일로 저장하였다.\n",
    "* Test Set은 트위터에서 서울 지하철에 대한 의견을 검색하여 긍정 10개, 부정 10개, 총 20개 문장을 MetroTestSet 텍스트 파일로 저장하였다.\n",
    "\n",
    "### 3-2. 나이브 베이즈 (NaiveBayesian)\n",
    "* 정서분류를 하기 위해 나이브 베이즈 기법을 사용하였다.\n",
    "* Machine Learning in Action 책에 나온 나이브 베이즈 함수를 수정하여 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MinJi C:\\Users\\MinJi\\Desktop/S_ParkMinJi/src/ C:\\Users\\MinJi\\Desktop/S_ParkMinJi/doc/\n"
     ]
    }
   ],
   "source": [
    "# directory setup\n",
    "import os\n",
    "myhome=os.path.expanduser('~')\n",
    "mywd=os.path.join(myhome,'Desktop/S_ParkMinJi/src/')\n",
    "mytxt=os.path.join(myhome,'Desktop/S_ParkMinJi/doc/')\n",
    "print myhome, mywd, mytxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MinJi\\Desktop\\S_ParkMinJi\\src\n"
     ]
    }
   ],
   "source": [
    "%cd {mywd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 서울지하철에 대한 관광객들의 Positive, Negative의견 단어 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "\"\"\"\n",
    "textParse\n",
    "input: bigString\n",
    "output: word list\n",
    "\"\"\"\n",
    "# 문자열 리스트로 텍스트를 구문 분석, 문자의 길이가 두 개 이하인 단어는 탈락, 모든 문자를 소문자로 변환\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "\"\"\"\n",
    "loadDataSet\n",
    "output: postingList, classVec\n",
    "\"\"\"\n",
    "def loadDataSet():\n",
    "    postingList = [];\n",
    "    # MetroPositiveTraining 파일을 줄 단위로 읽은 후, textParse를 거쳐 postingList에 저장한다.\n",
    "    with open('data/metro/MetroPositiveTraining.txt') as mPosTrain:\n",
    "        for i, line in enumerate(mPosTrain):\n",
    "            postingList.append(textParse(line))\n",
    "    classVec = list(zeros(150))    #0: Positive     \n",
    "    # MetroNegativeTraining 파일을 줄 단위로 읽은 후, textParse를 거쳐 postingList에 추가한다.    \n",
    "    with open('data/metro/MetroNegativeTraining.txt') as mNegTrain:\n",
    "        for i, line in enumerate(mNegTrain):\n",
    "            postingList.append(textParse(line))           \n",
    "    classVec.extend(ones(150))    #1: Negative\n",
    "    return postingList, classVec\n",
    "\n",
    "\"\"\"\n",
    "createVocabList\n",
    "input: dataSet\n",
    "output: list(vocabSet)\n",
    "\"\"\"\n",
    "# 모든 문서에 있는 유일한 단어 목록을 생성\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets - or\n",
    "    return list(vocabSet)\n",
    "\n",
    "\"\"\"\n",
    "createVocabList\n",
    "input: vocabList, inputSet\n",
    "output: returnVec\n",
    "\"\"\"\n",
    "#주어진 문서 내에 어휘 목록에 있는 단어가 존재하는지 아닌지를 표현 - 어휘 목록, 문서, 1과 0의 출력 데이터 사용\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) #어휘 목록과 같은 길이의 벡터를 생성하고 모두 0으로 채움\n",
    "    for word in inputSet: #문서 내에 있는 단어를 하나하나 비교\n",
    "        if word in vocabList: #해당 단어가 어휘 목록에 있다면\n",
    "            returnVec[vocabList.index(word)] = 1 #출력 벡터에 있는 해당 단어의 값을 1로 설정\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "# listPosts, listClasses = loadDataSet()\n",
    "# print listPosts, listClasses\n",
    "# myVocabList = createVocabList(listPosts)\n",
    "# print myVocabList\n",
    "# print listPosts[0]\n",
    "# print setOfWords2Vec(myVocabList, listPosts[0])\n",
    "# print listPosts[1]\n",
    "# print setOfWords2Vec(myVocabList, listPosts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나이브 베이즈 분류기 훈련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "trainNB0\n",
    "input: trainMatrix,trainCategory\n",
    "output: p0Vect,p1Vect,pAbusive\n",
    "\"\"\"\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "# listPosts, listClasses = loadDataSet()\n",
    "# myVocabList = createVocabList(listPosts)\n",
    "\n",
    "# trainMat = []\n",
    "# for postinDoc in listPosts:\n",
    "#     trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "\n",
    "# p0V,p1V,pAb=trainNB0(trainMat, listClasses)\n",
    "# print pAb\n",
    "# print p0V\n",
    "# print p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나이브 베이즈 분류 함수 및 테스트 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'convenient', 'all'] classified as:  1\n",
      "['the', 'underground', 'stations', 'are', 'not', 'staff', 'and', 'there', 'someone', 'the', 'information', 'kiosk'] classified as:  1\n",
      "['easy', 'accessible', 'great', 'running', 'times', 'regular', 'clean', 'english', 'signage', 'safe', 'modern'] classified as:  0\n",
      "['always', 'time', 'and', 'clean'] classified as:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "classifyNB\n",
    "input: vec2Classify, p0Vec, p1Vec, pClass1\n",
    "output: 0(positive) or 1(negative)\n",
    "\"\"\"\n",
    "# 0(positive) 또는 1(negative)로 분류\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\"\"\"\n",
    "testingNB\n",
    "input: text\n",
    "output: classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "\"\"\"    \n",
    "# 문자열 리스트를 받아 나이즈 베이즈 분류를 한 결과를 출력한다.\n",
    "def testingNB(text):\n",
    "    listPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "\n",
    "    testEntry = textParse(text)\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ', classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    return classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "\n",
    "# training data 중 임의로 네 가지를 골라 테스트 해보았음\n",
    "testingNB('Not convenient at all.')\n",
    "testingNB('The underground stations are not staff and there is someone on the information kiosk.')\n",
    "testingNB('Easy, accessible, great running times, regular, clean, English signage, safe, modern.')\n",
    "testingNB('Always on time and clean.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 나이브 베이즈 분류 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testData\n",
    "output: errorCount/testCount\n",
    "\"\"\"\n",
    "# testSet을 불러와 분류기를 실행시킨 후, 분류 결과와 기존 분류 결과를 비교하여 일치하지 않았을 경우 errorCount를 함\n",
    "# errorCount를 testCount(총 분류 시도 횟수)로 나누어 최종 에러율을 구함\n",
    "def testData():\n",
    "    testList = []; testClass = []; testCount = 0\n",
    "    with open('data/metro/MetroTestSet.txt') as mTest:\n",
    "        for i, line in enumerate(mTest):\n",
    "            currLine = line.strip().split('\\t')\n",
    "            testClass.extend(currLine[0])\n",
    "            testList.append(currLine[1])\n",
    "            testCount += 1         \n",
    "    errorCount = 0.0\n",
    "    for i in range(0, testCount):\n",
    "        if testingNB(testList[i]) != int(testClass[i]):\n",
    "            errorCount += 1\n",
    "        print 'origin class: ', int(testClass[i]), '  classified as: ', testingNB(testList[i])\n",
    "    print 'the error rate is:', errorCount/testCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  0   classified as:  0\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  0\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  1\n",
      "origin class:  1   classified as:  0\n",
      "origin class:  1   classified as:  1\n",
      "the error rate is: 0.1\n"
     ]
    }
   ],
   "source": [
    "import sentiment\n",
    "sentiment.testData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결론\n",
    "* 분류기의 오차율은 10%로, 가설에 대해 정확도는 90%임을 알 수 있었다.\n",
    "* Training 데이터로 사용된 트립어드바이저는 불편함에 대해 직접 언급하는 방면, Test 데이터인 트위터에는 비꼼, 이모티콘 등이 많이 사용되었다.\n",
    "* Training에 긍정, 부정으로 사용된 단어가 다양하였으므로 Training 데이터의 영향이 많이 줄었으나, 여전히 나이브 베이즈를 사용하기 위한 문장을 도출하는데 어려움이 있으며 Training 데이터에 영향을 많이 받는다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reference\n",
    "* https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98\n",
    "* http://www.opendataminer.com/sub/tutorial/naive.asp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
