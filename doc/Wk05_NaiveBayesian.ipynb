{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian\n",
    "* 나이브 베이즈 분류 (Naïve Bayes Classification)는 특성들 사이의 독립을 가정하는 베이즈 정리를 적용한 확률 분류기의 일종이다.\n",
    "* 나이브 베이즈는 분류기를 만드는 기술로써 단일 알고리즘을 통한 훈련이 아닌 일반적인 원칙에 근거한 여러 알고리즘들을 이용하여 훈련된다.\n",
    "* 모든 나이브 베이즈 분류기는 공통적으로 모든 특성 값은 서로 독립임을 가정한다. 예를 들어, 특정 과일을 사과로 분류 가능하게 하는 특성들 (둥글다, 빨갛다, 지름 10cm)은 나이브 베이즈 분류기에서 특성들 사이에서 발생할 수 있는 연관성이 없음을 가정하고 각각의 특성들이 특정 과일이 사과일 확률에 독립적으로 기여 하는 것으로 간주한다.\n",
    "\n",
    "### Baye's theorem\n",
    "* 두 확률 변수의 사전확률과 사후확률 사이의 관계를 나타내는 정리이다.\n",
    "* 베이즈 확률론 해석에 따르면 베이즈 정리는 새로운 근거가 제시될 때 사후 확률이 어떻게 갱신되는지를 구한다.\n",
    "* P(A|B) = P(B|A)P(A)/P(B)\n",
    "    * P(A|B) - 사건B가 발생한 상태에서 사건A가 발생할 조건부 확률\n",
    "    * P(B|A) - 사건A가 발생한 상태에서 사건B가 발생할 조건부 확률\n",
    "    * P(A) - 사건A가 발생할 확률, B에 대한 어떠한 정보도 없는 상태에서 A가 발생할 확률\n",
    "    * P(B) - 사건B가 발생할 확률, A에 대한 어떠한 정보도 없는 상태에서 B가 발생할 확률\n",
    "\n",
    "### 장점\n",
    "* It is easy and fast to predict class of test data set. It also perform well in multi class prediction.\n",
    "* When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "### 단점\n",
    "* If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
    "* On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "* Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "### 나이브 베이즈 분류를 위한 일반적인 접근 방식\n",
    "1. Collect: Any method. We’ll use RSS feeds in this chapter.\n",
    "2. Prepare: Numeric or Boolean values are needed.\n",
    "3. Analyze: With many features, plotting features isn’t helpful. Looking at histograms is a better idea.\n",
    "4. Train: Calculate the conditional probabilities of the independent features.\n",
    "5. Test: Calculate the error rate.\n",
    "6. Use: One common application of naïve Bayes is document classification. You can use naïve Bayes in any classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARKMINJI C:\\Users\\PARKMINJI\\Desktop/S_ParkMinJi/src/ C:\\Users\\PARKMINJI\\Desktop/S_ParkMinJi/doc/\n"
     ]
    }
   ],
   "source": [
    "# directory setup\n",
    "import os\n",
    "myhome=os.path.expanduser('~')\n",
    "mywd=os.path.join(myhome,'Desktop/S_ParkMinJi/src/')\n",
    "mytxt=os.path.join(myhome,'Desktop/S_ParkMinJi/doc/')\n",
    "print myhome, mywd, mytxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning in Action 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARKMINJI\\Desktop\\S_ParkMinJi\\src\n"
     ]
    }
   ],
   "source": [
    "%cd {mywd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 텍스트로 단어 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# 실험을 위해 몇 개의 예제 데이터를 생성\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "# 모든 문서에 있는 유일한 단어 목록을 생성\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets - or\n",
    "    return list(vocabSet)\n",
    "\n",
    "#주어진 문서 내에 어휘 목록에 있는 단어가 존재하는지 아닌지를 표현 - 어휘 목록, 문서, 1과 0의 출력 데이터 사용\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) #어휘 목록과 같은 길이의 벡터를 생성하고 모두 0으로 채움\n",
    "    for word in inputSet: #문서 내에 있는 단어를 하나하나 비교\n",
    "        if word in vocabList: #해당 단어가 어휘 목록에 있다면\n",
    "            returnVec[vocabList.index(word)] = 1 #출력 벡터에 있는 해당 단어의 값을 1로 설정\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import bayes\n",
    "from numpy import *\n",
    "\n",
    "listoPosts, listclasses = bayes.loadDataSet()\n",
    "myVocabList = bayes.createVocabList(listoPosts)\n",
    "print myVocabList\n",
    "print listoPosts[0]\n",
    "print bayes.setOfWords2Vec(myVocabList, listoPosts[0])\n",
    "print listoPosts[3]\n",
    "print bayes.setOfWords2Vec(myVocabList, listoPosts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count the number of documents in each class\n",
    "#for every training document: \n",
    "#    for each class: if a token appears in the document → increment the count for that token \n",
    "#    increment the count for tokens\n",
    "#for each class: \n",
    "#    for each token: \n",
    "#        divide the token count by the total token count to get conditional probabilities \n",
    "#return conditional probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    # 초기 확률 \n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) #사전확률 (=모욕적인 말이 나온 확률 ) Base Rate ,Frame theory, Anchor \n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)\n",
    "    p0Denom = 0.0; p1Denom = 0.0\n",
    "    # 벡터 추가 \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom\n",
    "    p0Vect = p0Num/p0Denom   \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.          0.          0.08333333  0.          0.\n",
      "  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667\n",
      "  0.04166667  0.125     ]\n",
      "[ 0.          0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.          0.05263158  0.05263158  0.          0.          0.\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.\n",
      "  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316\n",
      "  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "reload(bayes)\n",
    "listoPosts, listClasses = bayes.loadDataSet()\n",
    "myVocabList = bayes.createVocabList(listoPosts)\n",
    "\n",
    "trainMat = []\n",
    "for postinDoc in listoPosts:\n",
    "    trainMat.append(bayes.setOfWords2Vec(myVocabList, postinDoc))\n",
    "\n",
    "p0V,p1V,pAb=trainNB0(trainMat, listClasses)\n",
    "print pAb\n",
    "print p0V\n",
    "print p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts,listClasses = bayes.loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print thisDoc\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "reload(bayes)\n",
    "bayes.testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "print mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listofTokens = regEx.split(mySent)\n",
    "print listofTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print [tok.lower() for tok in listofTokens if len(tok)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OEM',\n",
       " 'Adobe',\n",
       " 'Microsoft',\n",
       " 'softwares',\n",
       " 'Fast',\n",
       " 'order',\n",
       " 'and',\n",
       " 'download',\n",
       " 'Microsoft',\n",
       " 'Office',\n",
       " 'Professional',\n",
       " 'Plus',\n",
       " '2007',\n",
       " '2010',\n",
       " '129',\n",
       " 'Microsoft',\n",
       " 'Windows',\n",
       " '7',\n",
       " 'Ultimate',\n",
       " '119',\n",
       " 'Adobe',\n",
       " 'Photoshop',\n",
       " 'CS5',\n",
       " 'Extended',\n",
       " 'Adobe',\n",
       " 'Acrobat',\n",
       " '9',\n",
       " 'Pro',\n",
       " 'Extended',\n",
       " 'Windows',\n",
       " 'XP',\n",
       " 'Professional',\n",
       " 'thousand',\n",
       " 'more',\n",
       " 'titles']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailText = open('data/email/spam/6.txt').read()\n",
    "listofTokens = regEx.split(emailText)\n",
    "listofTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARKMINJI\\Desktop\\S_ParkMinJi\\src\n"
     ]
    }
   ],
   "source": [
    "%cd {mywd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('data/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('data/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print \"classification error\",docList[docIndex]\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    #return vocabList,fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "reload(bayes)\n",
    "bayes.spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayesian 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARKMINJI\\Desktop\\S_ParkMinJi\\src\n"
     ]
    }
   ],
   "source": [
    "%cd {mywd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "\n",
    "ny['entries']\n",
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]       \n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.25\n",
      "the error rate is:  0.5\n"
     ]
    }
   ],
   "source": [
    "reload(bayes)\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "\n",
    "vocabList, pSF, pNY = bayes.localWords(ny,sf)\n",
    "vocabList, pSF, pNY = bayes.localWords(ny,sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.6\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "really\n",
      "massage\n",
      "how\n",
      "since\n",
      "girls\n",
      "friend\n",
      "need\n",
      "relationship\n",
      "married\n",
      "there\n",
      "woman\n",
      "near\n",
      "current\n",
      "body\n",
      "let\n",
      "alone\n",
      "change\n",
      "today\n",
      "more\n",
      "male\n",
      "times\n",
      "needs\n",
      "things\n",
      "hands\n",
      "kayaking\n",
      "each\n",
      "skate\n",
      "guy\n",
      "already\n",
      "open\n",
      "guess\n",
      "bet\n",
      "take\n",
      "situations\n",
      "should\n",
      "going\n",
      "hopefully\n",
      "stuff\n",
      "asleep\n",
      "reading\n",
      "both\n",
      "been\n",
      "search\n",
      "cant\n",
      "well\n",
      "yes\n",
      "interested\n",
      "know\n",
      "works\n",
      "down\n",
      "long\n",
      "lot\n",
      "happy\n",
      "trying\n",
      "nice\n",
      "friends\n",
      "time\n",
      "relieve\n",
      "month\n",
      "woods\n",
      "fan\n",
      "fall\n",
      "drinking\n",
      "cool\n",
      "skills\n",
      "reciprocation\n",
      "havent\n",
      "cmt\n",
      "heading\n",
      "enjoy\n",
      "says\n",
      "click\n",
      "drinks\n",
      "abd\n",
      "indian\n",
      "visting\n",
      "here\n",
      "slow\n",
      "mild\n",
      "boy\n",
      "involved\n",
      "thats\n",
      "crime\n",
      "from\n",
      "angry\n",
      "destination\n",
      "wondering\n",
      "knows\n",
      "company\n",
      "mellow\n",
      "car\n",
      "work\n",
      "learn\n",
      "meet\n",
      "give\n",
      "preferably\n",
      "watching\n",
      "toxic\n",
      "gaming\n",
      "lesbian\n",
      "man\n",
      "stress\n",
      "short\n",
      "childhood\n",
      "help\n",
      "trade\n",
      "playful\n",
      "still\n",
      "hayward\n",
      "fit\n",
      "acti\n",
      "willing\n",
      "break\n",
      "therapy\n",
      "now\n",
      "good\n",
      "goofy\n",
      "frriends\n",
      "went\n",
      "status\n",
      "everyone\n",
      "house\n",
      "year\n",
      "our\n",
      "saturday\n",
      "caucasian\n",
      "little\n",
      "teach\n",
      "care\n",
      "could\n",
      "skateboard\n",
      "thing\n",
      "first\n",
      "platonic\n",
      "relate\n",
      "lol\n",
      "playof\n",
      "hunt\n",
      "slept\n",
      "patna\n",
      "boring\n",
      "bed\n",
      "danc\n",
      "attached\n",
      "muir\n",
      "rent\n",
      "sat\n",
      "casual\n",
      "without\n",
      "finding\n",
      "wanting\n",
      "pain\n",
      "shall\n",
      "understands\n",
      "especially\n",
      "drive\n",
      "cheat\n",
      "sharks\n",
      "title\n",
      "stature\n",
      "years\n",
      "common\n",
      "hike\n",
      "through\n",
      "sex\n",
      "skateboarding\n",
      "close\n",
      "boundaries\n",
      "enjoys\n",
      "email\n",
      "wandering\n",
      "adventures\n",
      "games\n",
      "whatever\n",
      "learning\n",
      "expected\n",
      "hubbies\n",
      "oth\n",
      "else\n",
      "lives\n",
      "instructor\n",
      "myself\n",
      "straight\n",
      "single\n",
      "will\n",
      "sometime\n",
      "respected\n",
      "sudden\n",
      "grown\n",
      "funny\n",
      "pay\n",
      "same\n",
      "meetup\n",
      "split\n",
      "difficult\n",
      "mountain\n",
      "climbing\n",
      "has\n",
      "game\n",
      "either\n",
      "right\n",
      "old\n",
      "pals\n",
      "back\n",
      "skateboarder\n",
      "flirtatiou\n",
      "host\n",
      "anything\n",
      "road\n",
      "own\n",
      "female\n",
      "sports\n",
      "her\n",
      "forward\n",
      "mountains\n",
      "form\n",
      "offer\n",
      "jut\n",
      "skating\n",
      "wants\n",
      "laugh\n",
      "dog\n",
      "yoga\n",
      "gay\n",
      "sessions\n",
      "occasional\n",
      "other\n",
      "stay\n",
      "helping\n",
      "kinda\n",
      "together\n",
      "shop\n",
      "saying\n",
      "thoughtful\n",
      "porno\n",
      "buddy\n",
      "freelance\n",
      "young\n",
      "send\n",
      "charge\n",
      "helps\n",
      "smile\n",
      "friendly\n",
      "overpaying\n",
      "brown\n",
      "very\n",
      "wishes\n",
      "minded\n",
      "cook\n",
      "school\n",
      "purely\n",
      "list\n",
      "eve\n",
      "try\n",
      "discrete\n",
      "small\n",
      "entourage\n",
      "porn\n",
      "past\n",
      "rate\n",
      "street\n",
      "even\n",
      "giving\n",
      "new\n",
      "ever\n",
      "med\n",
      "9th\n",
      "never\n",
      "met\n",
      "healing\n",
      "2seconds\n",
      "strong\n",
      "great\n",
      "bible\n",
      "gent\n",
      "experience\n",
      "engine\n",
      "smoke\n",
      "opinion\n",
      "love\n",
      "secure\n",
      "loveless\n",
      "prefer\n",
      "bro\n",
      "bust\n",
      "post\n",
      "use\n",
      "nadie\n",
      "next\n",
      "few\n",
      "perv\n",
      "means\n",
      "relax\n",
      "successful\n",
      "haha\n",
      "anuncio\n",
      "beers\n",
      "basically\n",
      "excellent\n",
      "hold\n",
      "hola\n",
      "topic\n",
      "none\n",
      "word\n",
      "divorced\n",
      "soul\n",
      "movies\n",
      "cuddle\n",
      "del\n",
      "mechanic\n",
      "heart\n",
      "desnudo\n",
      "figure\n",
      "rejuvenated\n",
      "organized\n",
      "high\n",
      "unbiased\n",
      "something\n",
      "sense\n",
      "pedirle\n",
      "sit\n",
      "travel\n",
      "write\n",
      "amazing\n",
      "nyc\n",
      "tried\n",
      "haven\n",
      "aspiring\n",
      "may\n",
      "after\n",
      "birthday\n",
      "exhibitionist\n",
      "date\n",
      "such\n",
      "guys\n",
      "cuddling\n",
      "light\n",
      "every\n",
      "talk\n",
      "operations\n",
      "cute\n",
      "mainly\n",
      "soon\n",
      "london\n",
      "cold\n",
      "its\n",
      "before\n",
      "machine\n",
      "assplay\n",
      "group\n",
      "interesting\n",
      "better\n",
      "feel\n",
      "might\n",
      "dad\n",
      "then\n",
      "them\n",
      "sucking\n",
      "seeking\n",
      "conversations\n",
      "safe\n",
      "kiss\n",
      "day\n",
      "nor\n",
      "sucked\n",
      "term\n",
      "40s\n",
      "dabs\n",
      "documentary\n",
      "doing\n",
      "idea\n",
      "girl\n",
      "sexual\n",
      "special\n",
      "living\n",
      "blu\n",
      "diesel\n",
      "laid\n",
      "sane\n",
      "ass\n",
      "assured\n",
      "free\n",
      "que\n",
      "put\n",
      "weed\n",
      "26yo\n",
      "conversationalist\n",
      "honest\n",
      "unexcited\n",
      "american\n",
      "think\n",
      "grade\n",
      "feet\n",
      "hook\n",
      "done\n",
      "another\n",
      "city\n",
      "thirties\n",
      "top\n",
      "anyone\n",
      "their\n",
      "twenties\n",
      "white\n",
      "legs\n",
      "hug\n",
      "para\n",
      "gym\n",
      "park\n",
      "realized\n",
      "massages\n",
      "off\n",
      "flowers\n",
      "than\n",
      "loved\n",
      "kind\n",
      "require\n",
      "second\n",
      "decided\n",
      "ilnesses\n",
      "maybe\n",
      "feeling\n",
      "este\n",
      "missing\n",
      "gals\n",
      "pro\n",
      "ages\n",
      "mine\n",
      "hangout\n",
      "borough\n",
      "organs\n",
      "clearly\n",
      "kush\n",
      "able\n",
      "mid\n",
      "also\n",
      "which\n",
      "similiar\n",
      "latino\n",
      "blunt\n",
      "relaxed\n",
      "graduating\n",
      "early\n",
      "sure\n",
      "regular\n",
      "eaten\n",
      "tall\n",
      "nothing\n",
      "why\n",
      "hubby\n",
      "request\n",
      "definitely\n",
      "clean\n",
      "professional\n",
      "slope\n",
      "stoner\n",
      "show\n",
      "text\n",
      "estoy\n",
      "stoned\n",
      "rough\n",
      "busy\n",
      "unhappy\n",
      "only\n",
      "posted\n",
      "hope\n",
      "wearing\n",
      "watch\n",
      "incursionando\n",
      "reply\n",
      "gen\n",
      "catches\n",
      "artist\n",
      "bay\n",
      "naked\n",
      "cleavage\n",
      "grab\n",
      "greetings\n",
      "respond\n",
      "fascinating\n",
      "bucket\n",
      "see\n",
      "individual\n",
      "wonder\n",
      "repairs\n",
      "cents\n",
      "said\n",
      "movie\n",
      "currently\n",
      "please\n",
      "favor\n",
      "written\n",
      "won\n",
      "confianza\n",
      "between\n",
      "alguien\n",
      "notice\n",
      "men\n",
      "recently\n",
      "ability\n",
      "opening\n",
      "erotica\n",
      "attention\n",
      "subject\n",
      "wear\n",
      "swift\n",
      "limits\n",
      "april\n",
      "atleast\n",
      "alot\n",
      "necesito\n",
      "por\n",
      "active\n",
      "incest\n",
      "simply\n",
      "likes\n",
      "table\n",
      "fre\n",
      "ting\n",
      "confident\n",
      "beer\n",
      "seasoned\n",
      "motives\n",
      "meeting\n",
      "industria\n",
      "life\n",
      "educated\n",
      "relaxing\n",
      "mind\n",
      "smoking\n",
      "160\n",
      "those\n",
      "chill\n",
      "leave\n",
      "while\n",
      "situation\n",
      "explorement\n",
      "leaves\n",
      "skirts\n",
      "wouldn\n",
      "almost\n",
      "brow\n",
      "non\n",
      "seattle\n",
      "worry\n",
      "different\n",
      "fotos\n",
      "etc\n",
      "perhaps\n",
      "make\n",
      "gets\n",
      "week\n",
      "drink\n",
      "tops\n",
      "drives\n",
      "hand\n",
      "purpose\n",
      "fucked\n",
      "totally\n",
      "keep\n",
      "haga\n",
      "kept\n",
      "older\n",
      "smoker\n",
      "obviously\n",
      "thought\n",
      "person\n",
      "contact\n",
      "english\n",
      "chemistry\n",
      "money\n",
      "rest\n",
      "125th\n",
      "bite\n",
      "shape\n",
      "thanks\n",
      "cut\n",
      "had\n",
      "theater\n",
      "lets\n",
      "easy\n",
      "excited\n",
      "build\n",
      "real\n",
      "around\n",
      "ample\n",
      "big\n",
      "entrepreneurial\n",
      "dark\n",
      "lady\n",
      "knock\n",
      "necessary\n",
      "stranger\n",
      "vibrant\n",
      "strictly\n",
      "esto\n",
      "lose\n",
      "soft\n",
      "virginity\n",
      "often\n",
      "people\n",
      "understand\n",
      "spring\n",
      "neve\n",
      "publico\n",
      "humor\n",
      "bottom\n",
      "creative\n",
      "witty\n",
      "eso\n",
      "420\n",
      "thighs\n",
      "business\n",
      "asap\n",
      "marketing\n",
      "eye\n",
      "diagnostician\n",
      "about\n",
      "passionate\n",
      "lifestyle\n",
      "getting\n",
      "industry\n",
      "dinner\n",
      "act\n",
      "mixed\n",
      "wanna\n",
      "ppl\n",
      "ladies\n",
      "into\n",
      "errands\n",
      "two\n",
      "because\n",
      "soy\n",
      "perving\n",
      "story\n",
      "area\n",
      "support\n",
      "hey\n",
      "start\n",
      "low\n",
      "literal\n",
      "bored\n",
      "was\n",
      "head\n",
      "heal\n",
      "converse\n",
      "yeah\n",
      "spam\n",
      "hear\n",
      "line\n",
      "handsome\n",
      "again\n",
      "made\n",
      "smooth\n",
      "cruel\n",
      "verification\n",
      "knees\n",
      "pic\n",
      "doesn\n",
      "tengo\n",
      "proves\n",
      "chat\n",
      "curious\n",
      "check\n",
      "apartment\n",
      "mutual\n",
      "redwood\n",
      "when\n",
      "backs\n",
      "field\n",
      "details\n",
      "tomarme\n",
      "brothers\n",
      "felt\n",
      "breasts\n",
      "meaning\n",
      "brands\n",
      "home\n",
      "age\n",
      "hella\n",
      "rimmed\n",
      "far\n",
      "serious\n",
      "hello\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "good\n",
      "smoke\n",
      "few\n",
      "something\n",
      "off\n",
      "please\n",
      "educated\n",
      "single\n",
      "make\n",
      "well\n",
      "together\n",
      "young\n",
      "woman\n",
      "enjoy\n",
      "even\n",
      "great\n",
      "today\n",
      "company\n",
      "meet\n",
      "year\n",
      "sane\n",
      "que\n",
      "guy\n",
      "could\n",
      "gym\n",
      "massages\n",
      "than\n",
      "nothing\n",
      "professional\n",
      "sex\n",
      "favor\n",
      "necesito\n",
      "por\n",
      "situation\n",
      "drink\n",
      "has\n",
      "old\n",
      "marketing\n",
      "hey\n",
      "curious\n",
      "nice\n",
      "saying\n",
      "thoughtful\n",
      "porno\n",
      "send\n",
      "charge\n",
      "smile\n",
      "friendly\n",
      "very\n",
      "wishes\n",
      "minded\n",
      "drinking\n",
      "cool\n",
      "school\n",
      "list\n",
      "eve\n",
      "discrete\n",
      "heading\n",
      "porn\n",
      "past\n",
      "street\n",
      "giving\n",
      "current\n",
      "new\n",
      "body\n",
      "med\n",
      "9th\n",
      "here\n",
      "met\n",
      "alone\n",
      "strong\n",
      "bible\n",
      "gent\n",
      "experience\n",
      "massage\n",
      "love\n",
      "bro\n",
      "post\n",
      "from\n",
      "nadie\n",
      "next\n",
      "more\n",
      "relax\n",
      "successful\n",
      "anuncio\n",
      "beers\n",
      "hola\n",
      "topic\n",
      "word\n",
      "work\n",
      "del\n",
      "desnudo\n",
      "rejuvenated\n",
      "high\n",
      "sense\n",
      "pedirle\n",
      "write\n",
      "amazing\n",
      "aspiring\n",
      "may\n",
      "after\n",
      "birthday\n",
      "man\n",
      "things\n",
      "talk\n",
      "cute\n",
      "mainly\n",
      "london\n",
      "still\n",
      "its\n",
      "before\n",
      "machine\n",
      "assplay\n",
      "fit\n",
      "interesting\n",
      "better\n",
      "dad\n",
      "them\n",
      "sucking\n",
      "seeking\n",
      "conversations\n",
      "safe\n",
      "kiss\n",
      "hands\n",
      "nor\n",
      "sucked\n",
      "40s\n",
      "documentary\n",
      "doing\n",
      "our\n",
      "girl\n",
      "saturday\n",
      "living\n",
      "blu\n",
      "ass\n",
      "assured\n",
      "free\n",
      "put\n",
      "weed\n",
      "conversationalist\n",
      "unexcited\n",
      "american\n",
      "think\n",
      "first\n",
      "grade\n",
      "another\n",
      "thirties\n",
      "top\n",
      "anyone\n",
      "twenties\n",
      "white\n",
      "para\n",
      "park\n",
      "kind\n",
      "require\n",
      "attached\n",
      "este\n",
      "missing\n",
      "mine\n",
      "hangout\n",
      "borough\n",
      "clearly\n",
      "able\n",
      "mid\n",
      "also\n",
      "which\n",
      "latino\n",
      "relaxed\n",
      "graduating\n",
      "early\n",
      "regular\n",
      "eaten\n",
      "tall\n",
      "drive\n",
      "clean\n",
      "slope\n",
      "stoner\n",
      "text\n",
      "estoy\n",
      "unhappy\n",
      "incursionando\n",
      "reply\n",
      "gen\n",
      "married\n",
      "naked\n",
      "stuff\n",
      "grab\n",
      "greetings\n",
      "bucket\n",
      "see\n",
      "individual\n",
      "said\n",
      "confianza\n",
      "reading\n",
      "email\n",
      "alguien\n",
      "recently\n",
      "ability\n",
      "subject\n",
      "april\n",
      "alot\n",
      "likes\n",
      "been\n",
      "ting\n",
      "beer\n",
      "seasoned\n",
      "industria\n",
      "life\n",
      "mind\n",
      "straight\n",
      "will\n",
      "wouldn\n",
      "brow\n",
      "seattle\n",
      "different\n",
      "fotos\n",
      "perhaps\n",
      "gets\n",
      "week\n",
      "hand\n",
      "fucked\n",
      "totally\n",
      "haga\n",
      "older\n",
      "contact\n",
      "english\n",
      "chemistry\n",
      "money\n",
      "rest\n",
      "125th\n",
      "bite\n",
      "thanks\n",
      "cut\n",
      "had\n",
      "lets\n",
      "build\n",
      "real\n",
      "around\n",
      "big\n",
      "entrepreneurial\n",
      "know\n",
      "lady\n",
      "knock\n",
      "necessary\n",
      "vibrant\n",
      "esto\n",
      "soft\n",
      "often\n",
      "back\n",
      "publico\n",
      "humor\n",
      "bottom\n",
      "witty\n",
      "eso\n",
      "business\n",
      "asap\n",
      "host\n",
      "about\n",
      "passionate\n",
      "getting\n",
      "industry\n",
      "act\n",
      "wanna\n",
      "ladies\n",
      "into\n",
      "two\n",
      "down\n",
      "female\n",
      "soy\n",
      "her\n",
      "area\n",
      "there\n",
      "start\n",
      "literal\n",
      "bored\n",
      "converse\n",
      "yeah\n",
      "spam\n",
      "line\n",
      "handsome\n",
      "cruel\n",
      "verification\n",
      "pic\n",
      "tengo\n",
      "proves\n",
      "chat\n",
      "other\n",
      "really\n",
      "tomarme\n",
      "brothers\n",
      "felt\n",
      "meaning\n",
      "friends\n",
      "age\n",
      "rimmed\n",
      "serious\n",
      "shop\n",
      "relieve\n",
      "month\n",
      "woods\n",
      "buddy\n",
      "freelance\n",
      "helps\n",
      "overpaying\n",
      "brown\n",
      "fan\n",
      "fall\n",
      "cook\n",
      "purely\n",
      "skills\n",
      "try\n",
      "reciprocation\n",
      "small\n",
      "havent\n",
      "cmt\n",
      "entourage\n",
      "says\n",
      "rate\n",
      "click\n",
      "drinks\n",
      "abd\n",
      "near\n",
      "indian\n",
      "ever\n",
      "visting\n",
      "never\n",
      "slow\n",
      "let\n",
      "healing\n",
      "2seconds\n",
      "mild\n",
      "change\n",
      "boy\n",
      "involved\n",
      "engine\n",
      "opinion\n",
      "thats\n",
      "secure\n",
      "loveless\n",
      "prefer\n",
      "bust\n",
      "crime\n",
      "use\n",
      "angry\n",
      "destination\n",
      "perv\n",
      "wondering\n",
      "means\n",
      "knows\n",
      "haha\n",
      "basically\n",
      "excellent\n",
      "mellow\n",
      "hold\n",
      "none\n",
      "divorced\n",
      "car\n",
      "soul\n",
      "movies\n",
      "cuddle\n",
      "mechanic\n",
      "learn\n",
      "male\n",
      "heart\n",
      "figure\n",
      "give\n",
      "organized\n",
      "unbiased\n",
      "times\n",
      "preferably\n",
      "needs\n",
      "sit\n",
      "travel\n",
      "watching\n",
      "how\n",
      "toxic\n",
      "nyc\n",
      "gaming\n",
      "tried\n",
      "haven\n",
      "exhibitionist\n",
      "date\n",
      "such\n",
      "guys\n",
      "lesbian\n",
      "cuddling\n",
      "stress\n",
      "short\n",
      "light\n",
      "childhood\n",
      "every\n",
      "operations\n",
      "help\n",
      "soon\n",
      "trade\n",
      "playful\n",
      "cold\n",
      "group\n",
      "hayward\n",
      "acti\n",
      "feel\n",
      "willing\n",
      "might\n",
      "then\n",
      "break\n",
      "therapy\n",
      "now\n",
      "day\n",
      "term\n",
      "kayaking\n",
      "goofy\n",
      "each\n",
      "dabs\n",
      "frriends\n",
      "went\n",
      "skate\n",
      "status\n",
      "everyone\n",
      "house\n",
      "idea\n",
      "sexual\n",
      "special\n",
      "since\n",
      "diesel\n",
      "laid\n",
      "caucasian\n",
      "little\n",
      "teach\n",
      "care\n",
      "26yo\n",
      "skateboard\n",
      "honest\n",
      "thing\n",
      "already\n",
      "platonic\n",
      "relate\n",
      "feet\n",
      "hook\n",
      "done\n",
      "lol\n",
      "playof\n",
      "open\n",
      "city\n",
      "guess\n",
      "hunt\n",
      "slept\n",
      "girls\n",
      "their\n",
      "legs\n",
      "friend\n",
      "patna\n",
      "hug\n",
      "realized\n",
      "flowers\n",
      "loved\n",
      "boring\n",
      "second\n",
      "bed\n",
      "decided\n",
      "ilnesses\n",
      "maybe\n",
      "feeling\n",
      "danc\n",
      "bet\n",
      "gals\n",
      "pro\n",
      "muir\n",
      "ages\n",
      "rent\n",
      "need\n",
      "organs\n",
      "sat\n",
      "kush\n",
      "relationship\n",
      "casual\n",
      "without\n",
      "take\n",
      "finding\n",
      "wanting\n",
      "similiar\n",
      "blunt\n",
      "sure\n",
      "pain\n",
      "shall\n",
      "why\n",
      "understands\n",
      "hubby\n",
      "especially\n",
      "request\n",
      "definitely\n",
      "show\n",
      "cheat\n",
      "stoned\n",
      "rough\n",
      "sharks\n",
      "busy\n",
      "title\n",
      "situations\n",
      "should\n",
      "only\n",
      "going\n",
      "posted\n",
      "hope\n",
      "wearing\n",
      "stature\n",
      "hopefully\n",
      "watch\n",
      "years\n",
      "catches\n",
      "artist\n",
      "bay\n",
      "common\n",
      "hike\n",
      "cleavage\n",
      "through\n",
      "respond\n",
      "fascinating\n",
      "asleep\n",
      "skateboarding\n",
      "close\n",
      "wonder\n",
      "repairs\n",
      "cents\n",
      "movie\n",
      "currently\n",
      "boundaries\n",
      "written\n",
      "won\n",
      "between\n",
      "enjoys\n",
      "notice\n",
      "men\n",
      "opening\n",
      "erotica\n",
      "attention\n",
      "wear\n",
      "swift\n",
      "both\n",
      "limits\n",
      "wandering\n",
      "adventures\n",
      "games\n",
      "atleast\n",
      "whatever\n",
      "active\n",
      "incest\n",
      "simply\n",
      "learning\n",
      "table\n",
      "fre\n",
      "confident\n",
      "motives\n",
      "expected\n",
      "hubbies\n",
      "meeting\n",
      "oth\n",
      "relaxing\n",
      "search\n",
      "else\n",
      "lives\n",
      "smoking\n",
      "instructor\n",
      "160\n",
      "those\n",
      "chill\n",
      "myself\n",
      "leave\n",
      "while\n",
      "sometime\n",
      "explorement\n",
      "leaves\n",
      "skirts\n",
      "almost\n",
      "non\n",
      "respected\n",
      "cant\n",
      "sudden\n",
      "worry\n",
      "grown\n",
      "funny\n",
      "etc\n",
      "pay\n",
      "same\n",
      "meetup\n",
      "split\n",
      "difficult\n",
      "mountain\n",
      "tops\n",
      "drives\n",
      "purpose\n",
      "climbing\n",
      "keep\n",
      "kept\n",
      "smoker\n",
      "obviously\n",
      "thought\n",
      "person\n",
      "shape\n",
      "yes\n",
      "theater\n",
      "interested\n",
      "easy\n",
      "excited\n",
      "ample\n",
      "dark\n",
      "game\n",
      "stranger\n",
      "strictly\n",
      "either\n",
      "lose\n",
      "works\n",
      "virginity\n",
      "right\n",
      "people\n",
      "understand\n",
      "spring\n",
      "pals\n",
      "neve\n",
      "creative\n",
      "420\n",
      "skateboarder\n",
      "thighs\n",
      "flirtatiou\n",
      "eye\n",
      "diagnostician\n",
      "lifestyle\n",
      "anything\n",
      "dinner\n",
      "mixed\n",
      "road\n",
      "ppl\n",
      "own\n",
      "errands\n",
      "because\n",
      "sports\n",
      "perving\n",
      "story\n",
      "support\n",
      "long\n",
      "low\n",
      "lot\n",
      "forward\n",
      "was\n",
      "happy\n",
      "mountains\n",
      "head\n",
      "form\n",
      "offer\n",
      "heal\n",
      "hear\n",
      "jut\n",
      "trying\n",
      "again\n",
      "made\n",
      "smooth\n",
      "skating\n",
      "wants\n",
      "laugh\n",
      "knees\n",
      "doesn\n",
      "dog\n",
      "check\n",
      "apartment\n",
      "mutual\n",
      "yoga\n",
      "gay\n",
      "redwood\n",
      "sessions\n",
      "when\n",
      "backs\n",
      "occasional\n",
      "field\n",
      "details\n",
      "breasts\n",
      "stay\n",
      "helping\n",
      "kinda\n",
      "brands\n",
      "home\n",
      "hella\n",
      "time\n",
      "far\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "reload(bayes)\n",
    "bayes.getTopWords(ny,sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
